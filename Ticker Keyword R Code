
```{r}

library(tidyverse)
library(gtrendsR)
library(dplyr)
library(tidyr)
library(purrr)
library(broom)
library(zoo)
library(lubridate)
library(yahoofinancer)
library(data.table)
library(kableExtra)

```

```{r}
#removing noisy tickers
ambiguous_tickers <- c(
    "A", "ALL", "ARE", "BALL", "BEN", "BRO", "C", "CAT", "COST", "D", "DAY", "DECK", "DOC", "ED", "EW",
    "F", "FANG", "FAST", "FOX", "ICE", "IP", "IT", "J", "K", "KEY", "KEYS", "L", "MA", "MET", "MS", "NOW",
    "O", "ON", "SO", "T", "TAP", "V", "YUM", "COP", "GILD", "KIM", "LOW", "POOL", "TECH", "WELL", "HD", "HUM", "PSA", "USB"
)

#filtered dataset
unambiguous_ticker <- ticker_data2 %>%
    filter(!ticker %in% ambiguous_tickers)
```

TICKER
```{r}
# Step 1: Identify consecutive zero GSV runs per ticker
tickers <- ticker_data2 %>%
    arrange(ticker, date) %>%
    group_by(ticker) %>%
    mutate(
        zero_gsv = (GSV == 0),
        group_id = rleid(zero_gsv)  # assign group ids for consecutive values
    ) %>%
    group_by(ticker, group_id) %>%
    mutate(
        zero_run_length = ifelse(all(zero_gsv), n(), 0)
    ) %>%
    ungroup()

# Step 2: Find tickers with 8+ consecutive zero GSVs
tickers_to_remove2 <- tickers %>%
    group_by(ticker) %>%
    summarise(max_zero_run = max(zero_run_length, na.rm = TRUE)) %>%
    filter(max_zero_run >= 8) %>%
    pull(ticker)

# Step 3: Filter them out
ticker_filtered <- tickers %>%
    filter(!ticker %in% tickers_to_remove2) %>%
    select(-zero_gsv, -group_id, -zero_run_length)  # optional cleanup

#see how many tickers remain
n_distinct(ticker_filtered$ticker)

#471 stocks left after accounting for those not listed on S&P500 for entire period and those with 0 search volume.

```

TICKER 
```{r}
#calculating abnormal GSV.
ticker_data <- ticker_data2 %>%
    group_by(ticker) %>%
    arrange(date) %>%
    mutate(
        median_prev8 = lag(rollapply(GSV, width = 8, FUN = median, align = "right", fill = NA, na.rm = TRUE)),
        ASV = GSV - median_prev8
    ) %>%
    ungroup() %>%
  filter(!is.na(ASV)) #to remove NAs

```

TICKER
```{r}
#sort into weekly quantiles
ticker_data <- ticker_data %>%
  group_by(date) %>%
  mutate(ASV_quantile = ntile(ASV, 5)) %>%
  ungroup()

#summary stats table

summary_table_tic <- ticker_data %>%
  group_by(ASV_quantile) %>%
    summarise(
        Min_ASV = min(ASV, na.rm = TRUE),
        Max_ASV = max(ASV, na.rm = TRUE),
        Median_ASV = median(ASV, na.rm = TRUE),
        Mean_ASV = mean(ASV, na.rm = TRUE),
        SD = sd(ASV, na.rm = TRUE),
    ) %>%
    rename(
        `ASV Quantile` = ASV_quantile,
        `Min` = Min_ASV,
        `Max` = Max_ASV,
        `Median` = Median_ASV,
        `Mean` = Mean_ASV
    )

summary_table_tic %>%
    kable("html", digits = 2, caption = '<span style="color:black"> <b> Table 1: </b> Summary Statistics of ASV by Quantile <br> (Ticker Sample)</span>', align = "c") %>%
    kable_styling(full_width = FALSE, bootstrap_options = c("basic"))
```

```{r}
#calculating daily sp500 returns. 
daily_sp500 <- sp500_returns %>%
    group_by(tic) %>%
    arrange(datadate) %>%
    mutate(daily_return = (prccd - lag(prccd)) / lag(prccd)) %>%
    ungroup() %>%
    rename(ticker = tic, date = datadate) %>%
    filter(!is.na(daily_return)) #to remove NAs
```

TICKER
```{r}
#actual data
ticker_data <- ticker_data %>%
    mutate(
        portfolio_start = date + days(9),   # Next Tuesday because otherwise we have overlapping quartiles on Monday.
        portfolio_end = date + days(12)      # Monday after that
    )

tic_portfolio_daily <- ticker_data %>%
    rowwise() %>%
    mutate(date_list = list(seq(portfolio_start, portfolio_end, by = "1 day"))) %>%
    unnest(cols = c(date_list)) %>%
    select(ticker, ASV_quantile, portfolio_start, date = date_list)

tic_portfolio_daily_returns <- tic_portfolio_daily %>%
    left_join(daily_sp500, by = c("ticker", "date"))

tic_quartile_daily_returns <- tic_portfolio_daily_returns %>%
    group_by(date, ASV_quantile) %>%
    summarise(
        avg_return = mean(daily_return, na.rm = TRUE),
        .groups = "drop"
    ) %>%
  filter(!is.na(avg_return))

```


TICKER
```{r}
#FF regressions

#merge quartile return and FF datasets
tic_data <- tic_quartile_daily_returns %>%
  inner_join(FF, by = "date")

#get excess returns
tic_data <- tic_data %>%
  mutate(excess_return = avg_return - rf)

#run regression for each quartile
tic_regression_results <- tic_data %>%
  group_by(ASV_quantile) %>%
  nest() %>%
  mutate(model = map(data, ~ lm(excess_return ~ mktrf +hml+smb+umd, data = .x)),
         tidy_output = map(model, tidy),
         glance_output = map(model, glance))

#get regression results 
tic_regression_results %>%
  select(ASV_quantile, tidy_output) %>%
  unnest(tidy_output)

#r squared
tic_regression_results %>%
  select(ASV_quantile, glance_output) %>%
  unnest(glance_output)

```

```{r}
#average returns 
tic_data %>%
  group_by(ASV_quantile) %>%
  summarise(
    avg_daily_return = mean(excess_return, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    avg_daily_return = round(avg_daily_return * 100, 4),  # Convert to % and round
    ASV_quantile = case_when(
      ASV_quantile == 1 ~ "1 (lowest ASV)",
      ASV_quantile == 5 ~ "5 (highest ASV)",
      TRUE ~ as.character(ASV_quantile)  # Keep other quantiles as-is (2, 3, 4)
    )
  ) %>%
  kable("html",
        caption = '<span style="color:black"><b>Table 4:</b> Average Daily Excess Return by ASV Quantile (Ticker Sample) </span>',
        col.names = c("ASV Quantile", "Avg. Daily Excess Return (%)"),
        align = "c") %>%
  kable_styling(full_width = FALSE,
                bootstrap_options = c("basic"))
```

```{r}
target_stocks_tic <- c("A", "AAPL", "ABBV", "ABT", "ACN", "ADBE", "ADI", "ADM", "ADP", "ADSK", "AEE", "AEP", "AES", "AFL", "AIG", "AIZ", "AJG", "AKAM", "ALB", "ALGN", "ALL", "ALLE", "AMAT", "AMD", "AME", "AMGN", "AMP", "AMT", "AMZN", "ANET", "AON", "AOS", "APA", "APD", "APH", "APO", "ARE", "ATO", "AVB", "AVGO", "AVY", "AWK", "AXON", "AXP", "AZO", "BA", "BAC", "BALL", "BAX", "BBY", "BDX", "BEN", "BG", "BIIB", "BK", "BKNG", "BKR", "BLDR", "BLK", "BMY", "BR", "BRK.B", "BRO", "BSX", "BX", "BXP", "C", "CAG", "CAH", "CARR", "CAT", "CB", "CBOE", "CBRE", "CCI", "CCL", "CDNS", "CDW", "CF", "CFG", "CHD", "CHRW", "CHTR", "CI", "CINF", "CL", "CLX", "CMCSA", "CME", "CMG", "CMI", "CMS", "CNC", "CNP", "COF", "COO", "COP", "COR", "COST", "CPB", "CPRT", "CPT", "CRL", "CRM", "CRWD", "CSCO", "CSGP", "CSX", "CTAS", "CTRA", "CTSH", "CTVA", "CVS", "CVX", "CZR", "D", "DAL", "DAY", "DD", "DE", "DECK", "DELL", "DG", "DGX", "DHI", "DHR", "DIS", "DLR", "DLTR", "DOC", "DOV", "DOW", "DPZ", "DRI", "DTE", "DUK", "DVA", "DVN", "DXCM", "EA", "EBAY", "ECL", "ED", "EFX", "EG", "EIX", "EL", "ELV", "EMN", "EMR", "ENPH", "EOG", "EPAM", "EQIX", "EQR", "EQT", "ERIE", "ES", "ESS", "ETN", "ETR", "EW", "EXC", "EXPD", "EXPE", "EXR", "F", "FANG", "FAST", "FCX", "FDS", "FDX", "FE", "FFIV", "FI", "FICO", "FIS", "FITB", "FOX", "FRT", "FSLR", "FTNT", "FTV", "GD", "GDDY", "GE", "GEN", "GILD", "GIS", "GL", "GLW", "GM", "GNRC", "GOOG", "GOOGL", "GPC", "GPN", "GRMN", "GS", "GWW", "HAL", "HAS", "HBAN", "HCA", "HD", "HIG", "HII", "HLT", "HOLX", "HON", "HPE", "HPQ", "HRL", "HST", "HSY", "HUBB", "HUM", "HWM", "IBM", "ICE", "IDXX", "IEX", "IFF", "INCY", "INTC", "INTU", "INVH", "IP", "IPG", "IR", "IRM", "ISRG", "IT", "ITW", "IVZ", "J", "JBL", "JCI", "JNJ", "JPM", "K", "KDP", "KEY", "KEYS", "KHC", "KIM", "KKR", "KLAC", "KMB", "KMI", "KMX", "KO", "KR", "L", "LDOS", "LEN", "LH", "LHX", "LII", "LIN", "LKQ", "LLY", "LMT", "LNT", "LOW", "LRCX", "LULU", "LUV", "LVS", "LW", "LYB", "LYV", "MA", "MAA", "MAR", "MAS", "MCD", "MCHP", "MCK", "MCO", "MDLZ", "MDT", "MET", "META", "MGM", "MHK", "MKC", "MLM", "MMC", "MMM", "MNST", "MO", "MOH", "MOS", "MPC", "MPWR", "MRK", "MRNA", "MS", "MSCI", "MSFT", "MSI", "MTB", "MTCH", "MTD", "MU", "NCLH", "NDAQ", "NEE", "NEM", "NFLX", "NI", "NKE", "NOC", "NOW", "NRG", "NSC", "NTAP", "NTRS", "NUE", "NVDA", "NVR", "NWS", "NWSA", "NXPI", "O", "ODFL", "OKE", "OMC", "ON", "ORCL", "ORLY", "OTIS", "OXY", "PANW", "PARA", "PAYC", "PAYX", "PCAR", "PCG", "PEG", "PEP", "PFE", "PFG", "PG", "PGR", "PH", "PHM", "PKG", "PLD", "PM", "PNC", "PNR", "PNW", "PODD", "POOL", "PPG", "PPL", "PRU", "PSA", "PSX", "PTC", "PWR", "PYPL", "QCOM", "RCL", "REG", "REGN", "RF", "RJF", "RL", "RMD", "ROK", "ROL", "ROP", "ROST", "RSG", "RTX", "SBAC", "SBUX", "SCHW", "SHW", "SJM", "SLB", "SNA", "SNPS", "SO", "SPG", "SPGI", "SRE", "STE", "STLD", "STT", "STX", "STZ", "SWK", "SWKS", "SYF", "SYK", "SYY", "T", "TAP", "TDG", "TDY", "TECH", "TEL", "TER", "TFC", "TGT", "TJX", "TKO", "TMO", "TMUS", "TPL", "TPR", "TRGP", "TRMB", "TROW", "TRV", "TSCO", "TSLA", "TSN", "TT", "TTWO", "TXN", "TXT", "TYL", "UAL", "UBER", "UDR", "UHS", "ULTA", "UNH", "UNP", "UPS", "URI", "USB", "V", "VICI", "VLO", "VMC", "VRSK", "VRTX", "VST", "VTR", "VZ", "WAB", "WAT", "WBA", "WBD", "WDAY", "WDC", "WEC", "WELL", "WFC", "WM", "WMB", "WMT", "WRB", "WSM", "WST", "WTW", "WY", "WYNN", "XEL", "XOM", "XYL", "YUM", "ZBH", "ZBRA", "ZTS")

daily_sp500_subset_tic <- daily_sp500 %>%
  filter(ticker %in% target_stocks_tic) %>%
  inner_join(FF, by = "date")

buy_hold_daily_returns_tic <- daily_sp500_subset_tic %>%
  group_by(date) %>%
  summarise(avg_return = mean(daily_return, na.rm = TRUE)) %>%
  left_join(select(FF, date, rf), by = "date") %>%  # Explicitly join RF
  mutate(excess_return = avg_return - rf) %>%        # Subtract the correct RF
  arrange(date)

buy_hold_cumulative_tic <- buy_hold_daily_returns_tic %>%
  mutate(
    cum_excess_return = cumprod(1 + excess_return) - 1,
    ASV_quantile = "Equal-Weighted"
  ) %>%
  select(date, ASV_quantile, cum_excess_return)
```

```{r}
#plotting cumulative returns of quantile portfolios, buy-and-hold and long-short strategy. 

tic_cumulative_returns <- tic_quartile_daily_returns %>%
  left_join(select(FF, date, rf), by = "date") %>%
  arrange(ASV_quantile, date) %>%
  group_by(ASV_quantile) %>%
  mutate(
    excess_return = avg_return - rf,  # Subtract risk-free rate
    cum_excess_return = cumprod(1 + excess_return) - 1  # Compound excess returns
  ) %>%
  filter(!is.na(cum_excess_return))

combined_cumulative_tic <- bind_rows(
    tic_cumulative_returns %>% mutate(ASV_quantile = as.character(ASV_quantile)),
    buy_hold_cumulative_tic,
    tic_regression_data %>% 
      select(date, cum_excess_return = cum_return) %>% 
      mutate(ASV_quantile = "Long-Short")
) %>%
    mutate(ASV_quantile = factor(
        ASV_quantile, 
        levels = c("Equal-Weighted", "1", "2", "3", "4", "5", "Long-Short")
    ))

#FINAL cumulative return graph
ggplot(combined_cumulative_tic, aes(x = date, y = cum_excess_return, color = ASV_quantile)) +
    geom_line(size = 1) +
    labs(
        x = "Date",
        y = "Cumulative Excess Return",
        color = "Strategy"
    ) +
    ggtitle(bquote(bold("Figure 3")*": Cumulative Excess Returns for ASV-based Strategies (Ticker Keyword Sample)")) +
    scale_y_continuous(labels = scales::percent) +
    scale_color_discrete(
        labels = c("Equal-Weighted" = "Equal-Weighted (buy-and-hold)",
                  "1" = "Long Q1 (lowest ASV)",
                  "2" = "Long Q2",
                  "3" = "Long Q3",
                  "4" = "Long Q4",
                  "5" = "Long Q5 (highest ASV)",
                  "Long-Short" = "Long-Short (Q1-Q5)")
    ) +
    theme_minimal()
```

```{r}
#plotting cumulative returns

tic_cumulative_returns <- tic_quartile_daily_returns %>%
    arrange(ASV_quantile, date) %>%
    group_by(ASV_quantile) %>%
    mutate(cum_return = cumprod(1 + avg_return) - 1)

# Plot cumulative returns
ggplot(tic_cumulative_returns, aes(x = date, y = cum_return, color = as.factor(ASV_quantile))) +
    geom_line(size = 1) +
    labs(
        title = "Cumulative Return per ASV Quartile",
        x = "Date",
        y = "Cumulative Return",
        color = "ASV Quantile"
    ) +
    theme_minimal()

#1 means 100%, 1.5 means 150%
```

```{r}
# Extract coefficients 
tic_coef_table <- tic_regression_results %>%
  select(ASV_quantile, tidy_output) %>%
  unnest(tidy_output)

# Extract R-squared
tic_rsq_table <- tic_regression_results %>%
  select(ASV_quantile, glance_output) %>%
  unnest(glance_output) %>%
  select(ASV_quantile, r.squared, adj.r.squared)

# Join and clean up
tic_final_table <- tic_coef_table %>%
  left_join(cn_rsq_table, by = "ASV_quantile") %>%
  mutate(across(where(is.numeric), round, 4)) %>%
  arrange(ASV_quantile, term)

# Keep original p-values
tic_final_table <- tic_coef_table %>%
  left_join(cn_rsq_table, by = "ASV_quantile") %>%
  arrange(ASV_quantile, term) %>%
  rename(
    Quantile = ASV_quantile,
    Term = term,
    Estimate = estimate,
    `Std. Error` = std.error,
    Statistic = statistic,
    `P-Value` = p.value,
    `R²` = r.squared,
    `Adj. R²` = adj.r.squared
  )

tic_final_table %>%
  mutate(
    Estimate = round(Estimate, 3),
    `Std. Error` = round(`Std. Error`, 4),
    Statistic = round(Statistic, 3),
    `R²` = round(`R²`, 2),
    `Adj. R²` = round(`Adj. R²`, 2),
    `P-Value` = ifelse(
      `P-Value` < 0.05,
      paste0("<strong>", formatC(`P-Value`, format = "f", digits = 4), "</strong>"),
      formatC(`P-Value`, format = "f", digits = 4)
    )
  ) %>%
  kable("html",
        escape = FALSE,
        caption = '<span style="color:black"><b>Table 6:</b> Fama-French Regression Results by ASV Quantile (Ticker Sample)</span>',
        align = "c") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("condensed"))
```

```{r}
#only alpha coefficient regression table
library(dplyr)
library(tidyr)
library(knitr)
library(kableExtra)

# --- Coefficient and stats tables ---
tic_coef_table <- tic_regression_results %>%
  select(ASV_quantile, tidy_output) %>%
  unnest(tidy_output)

tic_intercepts <- tic_coef_table %>%
  filter(term == "(Intercept)") %>%
  select(ASV_quantile, estimate, std.error, statistic, p.value)

tic_stats <- tic_regression_results %>%
  select(ASV_quantile, glance_output) %>%
  unnest(glance_output) %>%
  select(ASV_quantile, r.squared, statistic, p.value) %>%
  rename(F_statistic = statistic, p_value_F = p.value)

# --- Join, format, and style ---
tic_final_table <- tic_intercepts %>%
  left_join(tic_stats, by = "ASV_quantile") %>%
  mutate(
    Quantile = case_when(
      ASV_quantile == 1 ~ "1 (Low ASV)",
      ASV_quantile == 5 ~ "5 (High ASV)",
      TRUE ~ as.character(ASV_quantile)
    ),
    Alpha_disp   = formatC(estimate,    format = "f", digits = 4),
    SE_disp      = formatC(std.error,   format = "f", digits = 4),
    t_disp       = formatC(statistic,   format = "f", digits = 3),
    pA_disp      = formatC(p.value,     format = "f", digits = 4),
    R2_disp      = formatC(r.squared,   format = "f", digits = 4),
    F_disp       = formatC(F_statistic, format = "f", digits = 2),
    pF_disp      = formatC(p_value_F,   format = "f", digits = 4),
    # Style Alpha p-values: bold if significant
    `P-Value (Alpha)` = ifelse(p.value < 0.05,
                               paste0("<strong>", pA_disp, "</strong>"),
                               pA_disp),
    # Style F-test p-values: bold if significant
    `P-Value (F)` = ifelse(p_value_F < 0.05,
                           paste0("<strong>", pF_disp, "</strong>"),
                           pF_disp)
  ) %>%
  select(
    Quantile,
    Alpha = Alpha_disp,
    `Std. Error` = SE_disp,
    `t-Statistic` = t_disp,
    `P-Value (Alpha)`,
    `R²` = R2_disp,
    `F Statistic` = F_disp,
    `P-Value (F)`
  )

# --- Render table ---
tic_final_table %>%
  kable(
    "html",
    escape = FALSE,
    caption = '<span style="color:black"><b>Table 6:</b> Fama–French Regression Results for Ticker Sample (Intercepts and Model Statistics)</span>',
    align = "c"
  ) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("condensed"))

```

```{r}
#long-short returns
tic_long_short_returns <- tic_quartile_daily_returns %>%
    filter(ASV_quantile %in% c(5, 1)) %>%
    select(date, ASV_quantile, avg_return) %>%
    pivot_wider(names_from = ASV_quantile, values_from = avg_return, names_prefix = "Q") %>%
    mutate(long_short = Q1 - Q5)

#merge with FF data
tic_regression_data <- tic_long_short_returns %>%
  inner_join(FF, by = "date") %>%
  mutate(long_short_excess = long_short - rf,    
         cum_return = cumprod(1 + long_short_excess) - 1)

#run regressions
tic_model <- lm(long_short_excess ~ mktrf + smb + hml + umd, data = tic_regression_data)
summary(tic_model)

# Extract coefficient details
tic_long_short_coef <- tidy(tic_model)

# Extract model summary statistics
tic_long_short_rsq <- glance(tic_model)

tic_long_short_table <- tic_long_short_coef %>%
  mutate(Quantile = "Q1 - Q5") %>%  # Label for display
  select(Quantile, term, estimate, std.error, statistic, p.value) %>%
  rename(
    Term = term,
    Estimate = estimate,
    `Std. Error` = std.error,
    Statistic = statistic,
    `P-Value` = p.value
  ) %>%
  mutate(
    `R²` = round(tic_long_short_rsq$r.squared, 2),
    `F-statistic` = round(tic_long_short_rsq$statistic, 2),
    `F P-Value` = ifelse(
      tic_long_short_rsq$p.value < 0.05,
      paste0("<strong>", formatC(tic_long_short_rsq$p.value, format = "f", digits = 4), "</strong>"),
      formatC(tic_long_short_rsq$p.value, format = "f", digits = 4)
    )
  ) %>%
  mutate(
    `F-statistic` = ifelse(row_number() == 1, `F-statistic`, ""),
    `F P-Value` = ifelse(row_number() == 1, `F P-Value`, "")
  )

# Step 7: Display nicely with kable
tic_long_short_table %>%
  mutate(
    Estimate = round(Estimate, 4),
    `Std. Error` = round(`Std. Error`, 4),
    Statistic = round(Statistic, 3),
    `P-Value` = ifelse(
      `P-Value` < 0.05,
      paste0("<strong>", formatC(`P-Value`, format = "f", digits = 4), "</strong>"),
      formatC(`P-Value`, format = "f", digits = 4)
    )
  ) %>%
  kable("html",
        escape = FALSE,
        caption = '<span style="color:black"><b>Table 8:</b> Fama-French Regression for Long–Short Portfolio (Q1 – Q5) (Ticker Keyword Sample) </span>',
        align = "c") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("condensed"))
```

RECYCLING
```{r}  
company_lookup <- data.frame(
  ticker = tickers <- c("ADM", "AEP", "ANET", "ATO", "BDX", "CAG", "CEG", "CL", "CPT", "CSCO", "DD", "DPZ", "DRI", "DTE", "EA", "ED", "EMN", "ENPH", "EOG", "EPAM", "ES", "FE", "FICO", "FITB", "GPC", "GWW", "HCA", "HII", "HPE", "HRL", "ICE", "IPG", "J", "K", "LVS", "LYB", "MCO", "MGM", "NRG", "NXPI", "ODFL", "PKG", "SNA", "SWKS", "TAP", "TMO", "TT", "TTWO", "VMC", "WYNN"),

  company_name = ctickers <- c("ADM stock", "AEP stock", "arista stock", "atmos stock", "BD stock", "conagra stock", "constellation stock", "colgate stock", "camden stock", "cisco stock", "dupont stock,", "dominos stock", "darden stock", "DTE stock", "EA stock", "conEd stock", "eastman stock", "enphase stock", "eog stock", "EPAM stock", "eversource stock", "first energy stock", "fico stock", "Fifth Third Bank stock", "GPC stock", "grainger stock", "HCA stock", "HII stock", "HPE stock", "hormel stock", "ICE stock", "IPG stock", "jacobs stock", "kellogg stock", "sands stock", "LYB stock", "moodys stock", "mgm stock", "nrg stock", "NXP stock", "ODFL stock", "PCA stock", "snap on stock", "skyworks stock", "coors stock", "thermo fisher stock", "trane stock", "take two stock", "vulcan stock", "wynn stock"))

# Empty dataframe
gsv_cn <- data.frame()

# Loop with lookup for company name
for (ticker in tickers) {
  # Lookup company name for search
  company_name <- company_lookup$company_name[company_lookup$ticker == ticker]
  keyword <- company_name  # ✅ Use company name as the search term

  message("Fetching: ", keyword)

  tryCatch({
    res <- gtrends(
      keyword = keyword,
      geo = "US",
      time = "2020-05-01 2025-07-01",
      gprop = "web",
      onlyInterest = TRUE
    )

    df <- res$interest_over_time
    df$date <- as.Date(df$date)
    df$ticker <- ticker
    df$company_name <- company_name  # ✅ Keep company name in results
    gsv_cn <- bind_rows(gsv_cn, df)

    Sys.sleep(2)  # Avoid Google API blocking
  }, error = function(e) {
    message("Error with ", ticker, ": ", e$message)
  })
}

# Rename 'hits' to 'GSV' and drop unused columns
names(gsv_cn)[names(gsv_cn) == "hits"] <- "GSV"
gsv_cn <- gsv_cn[, !(names(gsv_cn) %in% c("gprop", "category"))]
```

